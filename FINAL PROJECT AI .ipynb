{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5232dc",
   "metadata": {},
   "source": [
    "# APPENDIX A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88282a8c",
   "metadata": {},
   "source": [
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c5778d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib\n",
    "import re\n",
    "import gensim\n",
    "import nltk.tokenize\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf696b",
   "metadata": {},
   "source": [
    "# Building a corpus with reddit\n",
    "For scraping our reddit data i have used PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770fe7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (7.5.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (from praw) (1.3.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (from prawcore<3,>=2.1->praw) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mithunprithvi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3d90b",
   "metadata": {},
   "source": [
    "creating a corpus with subredditname.txt from a particular subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ead121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = \"vDe3LCe2FfUK7P_Z2Kr2_Q\" #your client id\n",
    "cs = \"18S93tUkP4xQbWMFBAtTpgzvgA6a5A\" #your client secret\n",
    "ua = \"tha\" #your user agent name\n",
    "sub = \"MensRights\" #the name of the subreddit (not including the 'r/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7587f8",
   "metadata": {},
   "source": [
    "subredditname.txt file containing post titles, content and top level comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=ci,\n",
    "    client_secret=cs,\n",
    "    user_agent=ua\n",
    ")\n",
    "\n",
    "with open(sub+\".txt\", \"w\") as f:\n",
    "    \n",
    "    #on the following line you can change top to any of the previously mentioned ways of sorting \n",
    "    #and the limit to however many posts you would like to extract (here we extract just 10).\n",
    "    for post in reddit.subreddit(sub).top(limit=10): \n",
    "        \n",
    "        #this line collects the post titles\n",
    "        f.write(post.title+\"\\n\")\n",
    "        \n",
    "        #this line collects the post content\n",
    "        f.write(post.selftext+\"\\n\")\n",
    "        \n",
    "        #this section collects the comments\n",
    "        for comment in post.comments:\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "            f.write(comment.body+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39259b49",
   "metadata": {},
   "source": [
    "Loading the .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121a2457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How to get banned from r/Feminism\\n', '\\n', \"This kind of post is normally removed as it violates our subreddit policies. However, I have been travelling and didn't see it. At this point, with the number of participants and comments, removing it would also have a detrimental effect to that valuable discussion. As a result, I will be leaving this up.\\n\", '\\n', \"Those who continue to report it won't be satisfied with the response.\\n\", 'It looks like they are behind on banning people. Usually [this](http://imgur.com/MAqsCnN) is enough.\\n', '\"Feeling safe\" is how we ended up with the TSA and their useless security theatre. \\n', '[deleted]\\n', \"Female 50 here, I tend to agree that ensuring people are safe is more important than trying to make them feel safe.  I think being safe is a generic male and female, adult and child, issue but what makes someone feel safe is an individual issue.  I don't think we can guarantee anyone's safety but we should strive to create a society where everyone is valued and in that valuation people are made safe because if you value something you tend to protect it and keep it safe.  I think it was inappropriate to ban you for raising a point of view which was not particularly offensive, derogatory or anti women.\\n\", 'I don\\'t actually agree with what OP says in his argument. However, it is a valid argument and I don\\'t see why he should be banned for it. So I tried to post a PNG of this, with the title \"Is it right to ban people we don\\'t agree with\". Guess what? I got banned.\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('MensRights.txt') as text:\n",
    "     lines = text.readlines()\n",
    "print(lines[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9480cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to get banned from r/Feminism\n",
      "\n",
      "\n",
      "\n",
      "This kind of post is normally removed as it violates our subreddit policies. However, I have been travelling and didn't see it. At this point, with the number of participants and comments, removing it would also have a detrimental effect to that valuable discussion. As a result, I will be leaving this up.\n",
      "\n",
      "\n",
      "\n",
      "Those who continue to report it won't be satisfied with the response.\n",
      "\n",
      "It looks like they are behind on banning people. Usually [this](http://imgur.com/MAqsCnN) is enough.\n",
      "\n",
      "\"Feeling safe\" is how we ended up with the TSA and their useless security theatre. \n",
      "\n",
      "[deleted]\n",
      "\n",
      "Female 50 here, I tend to agree that ensuring people are safe is more important than trying to make them feel safe.  I think being safe is a generic male and female, adult and child, issue but what makes someone feel safe is an individual issue.  I don't think we can guarantee anyone's safety but we should strive to create a society where everyone is valued and in that valuation people are made safe because if you value something you tend to protect it and keep it safe.  I think it was inappropriate to ban you for raising a point of view which was not particularly offensive, derogatory or anti women.\n",
      "\n",
      "I don't actually agree with what OP says in his argument. However, it is a valid argument and I don't see why he should be banned for it. So I tried to post a PNG of this, with the title \"Is it right to ban people we don't agree with\". Guess what? I got banned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in lines[:10]:\n",
    "    print (l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a3653",
   "metadata": {},
   "source": [
    "recognize punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb133a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\n",
    "def is_punct(string):\n",
    "    return PUNCT_RE.match(string) is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9153d57",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f11744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'to', 'get', 'banned', 'from', 'r/Feminism']\n",
      "[]\n",
      "['This', 'kind', 'of', 'post', 'is', 'normally', 'removed', 'as', 'it', 'violates', 'our', 'subreddit', 'policies', 'However', 'I', 'have', 'been', 'travelling', 'and', 'did', \"n't\", 'see', 'it', 'At', 'this', 'point', 'with', 'the', 'number', 'of', 'participants', 'and', 'comments', 'removing', 'it', 'would', 'also', 'have', 'a', 'detrimental', 'effect', 'to', 'that', 'valuable', 'discussion', 'As', 'a', 'result', 'I', 'will', 'be', 'leaving', 'this', 'up']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "for l in lines[:4]:\n",
    "    tokens = nltk.tokenize.word_tokenize(l)\n",
    "    tokens_processed = [token for token in tokens if not is_punct(token)]\n",
    "    print(tokens_processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aaf950",
   "metadata": {},
   "source": [
    "processing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5006618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'to', 'get', 'banned', 'from', 'r/Feminism']\n",
      "['How', 'to', 'get', 'banned', 'from', 'r/Feminism']\n"
     ]
    }
   ],
   "source": [
    "for l in lines[:1]:\n",
    "    tokens = nltk.tokenize.word_tokenize(l)\n",
    "    print(tokens)\n",
    "    \n",
    "    tokens_processed = [token for token in tokens if not is_punct(token)]\n",
    "    print(tokens_processed)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc22753",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for l in lines[:2000]:\n",
    "    tokens = nltk.tokenize.word_tokenize(l)\n",
    "    tokens_processed = [token for token in tokens if not is_punct(token)]\n",
    "      print(tokens_processed)\n",
    "    data.append(tokens_processed)\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87510c",
   "metadata": {},
   "source": [
    "# importing the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c46114",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(data, min_count=1)\n",
    "model.save('sample_model.bin')\n",
    "sample_model = gensim.models.Word2Vec.load('sample_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f94b14",
   "metadata": {},
   "source": [
    "similar words using word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b0235a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', 0.9991027116775513),\n",
       " ('my', 0.9990931153297424),\n",
       " ('the', 0.9990569353103638),\n",
       " ('a', 0.9990565776824951),\n",
       " ('an', 0.9990531206130981),\n",
       " ('people', 0.9990408420562744),\n",
       " ('about', 0.9990378022193909),\n",
       " ('me', 0.9990372657775879),\n",
       " ('their', 0.9990353584289551),\n",
       " ('this', 0.999026358127594)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38dcf0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is', 0.9623606204986572),\n",
       " ('common', 0.9619383811950684),\n",
       " ('looks', 0.9618855714797974),\n",
       " ('person', 0.9616388082504272),\n",
       " ('need', 0.9615545272827148),\n",
       " ('posts', 0.9614678025245667),\n",
       " ('case', 0.9613025784492493),\n",
       " ('fucking', 0.9612689018249512),\n",
       " ('whole', 0.9610376954078674),\n",
       " ('13', 0.9609751105308533)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.wv.most_similar(\"inappropriate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c646d626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Something', 0.899318516254425),\n",
       " ('seat', 0.8980953693389893),\n",
       " ('door', 0.8960670828819275),\n",
       " ('true', 0.8959680199623108),\n",
       " ('fun', 0.8952599167823792),\n",
       " ('sex', 0.8939398527145386),\n",
       " ('tell', 0.8937223553657532),\n",
       " ('run', 0.8935520052909851),\n",
       " ('clothes', 0.8934857249259949),\n",
       " ('tie', 0.8933970928192139)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.wv.most_similar(\"offensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d204705",
   "metadata": {},
   "source": [
    "creating review model to analyse using tensorflow projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ec1ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_model = gensim.models.Word2Vec(data, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a51dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('making', 0.9760643243789673),\n",
       " ('legs', 0.9758474230766296),\n",
       " ('If', 0.9755975604057312),\n",
       " ('judge', 0.975551426410675),\n",
       " ('getting', 0.9753536581993103),\n",
       " ('post', 0.9753187894821167),\n",
       " ('taken', 0.9751673340797424),\n",
       " ('common', 0.9750597476959229),\n",
       " ('even', 0.9750344157218933),\n",
       " ('issue', 0.9750192165374756)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_model.wv.most_similar('safety')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ee77f",
   "metadata": {},
   "source": [
    "this is how word is represented in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3237f8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.01329990e-02,  2.10773066e-01, -2.48651323e-03,  1.51297450e-02,\n",
       "        2.34836154e-02, -3.61417145e-01,  2.09395766e-01,  3.48338366e-01,\n",
       "       -2.06311390e-01, -2.47152030e-01, -5.87382242e-02, -3.01042885e-01,\n",
       "       -2.36334391e-02,  9.38113183e-02,  5.08838147e-02, -4.13921997e-02,\n",
       "        2.56484561e-02, -1.37601152e-01, -6.23113811e-02, -3.86226535e-01,\n",
       "        7.56585523e-02,  1.27175823e-01,  1.63540214e-01, -7.18095675e-02,\n",
       "       -7.15657920e-02,  8.29698443e-02, -1.41722351e-01, -6.30511642e-02,\n",
       "       -1.74599275e-01,  2.08262019e-02,  2.42789745e-01,  2.40952671e-02,\n",
       "        1.70688406e-01, -1.47744685e-01, -4.07311097e-02,  2.24938363e-01,\n",
       "        1.01345323e-01, -1.33512124e-01, -1.00105353e-01, -3.42063636e-01,\n",
       "        1.37130767e-01, -1.86752751e-01, -1.52644992e-01, -6.44861236e-02,\n",
       "        2.10366681e-01, -5.59841953e-02, -1.22812279e-01, -1.18444040e-01,\n",
       "        1.72601417e-01,  3.67967971e-02,  1.77569881e-01, -1.73416197e-01,\n",
       "        9.87257063e-03, -6.76762164e-02, -1.08893290e-01,  1.05372831e-01,\n",
       "        1.14813045e-01,  9.54165962e-03, -1.51849508e-01,  5.38847968e-02,\n",
       "        8.50493237e-02,  3.17023369e-04,  1.08267613e-01, -6.24321140e-02,\n",
       "       -2.61273950e-01,  1.80632547e-01,  9.46797803e-03,  1.13589481e-01,\n",
       "       -3.37230325e-01,  2.38024697e-01, -1.78475752e-01,  1.36574626e-01,\n",
       "        1.27917096e-01, -2.37024333e-02,  2.50008941e-01,  1.50591627e-01,\n",
       "       -2.99268030e-02,  1.50386626e-02, -1.00964159e-01,  1.34151326e-02,\n",
       "       -8.10654536e-02,  5.21278158e-02, -1.94162130e-01,  3.11731994e-01,\n",
       "       -1.45190582e-01, -5.82070425e-02,  1.49659961e-01,  1.15277290e-01,\n",
       "        2.49697864e-01,  1.09902650e-01,  2.06905052e-01,  7.51410127e-02,\n",
       "        7.16135129e-02,  4.10254821e-02,  3.17825496e-01,  1.80687457e-01,\n",
       "        1.76218033e-01, -1.29649326e-01,  8.12254846e-02, -3.22268233e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_model.wv['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2b29e",
   "metadata": {},
   "source": [
    "converting the file to .bin to use export to metadata and tensor data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd6ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_model2 = gensim.models.Word2Vec(data, min_count=1)\n",
    "\n",
    "review_model2.wv.save_word2vec_format('review_model2.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383c079",
   "metadata": {},
   "source": [
    "# The code for exporting the metadata and tensor data for projections:\n",
    "python -m gensim.scripts.word2vec2tensor -i /Users/mithunprithvi/Downloads/AI/review_model2.bin -o /Users/mithunprithvi/Downloads/AI/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b8520",
   "metadata": {},
   "source": [
    "Building a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0b93e",
   "metadata": {},
   "source": [
    "Training the model using the nltk n-gram pipeline and here 3 means 3- \"n-grams\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4f79ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "train, vocab = padded_everygram_pipeline(3, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb743c8",
   "metadata": {},
   "source": [
    "Using maximum likelihood estimator in language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f42be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = MLE(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd3118",
   "metadata": {},
   "source": [
    " Fit the model to generate the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f1e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 4707 items>\n"
     ]
    }
   ],
   "source": [
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b59d060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'they',\n",
       " 'are',\n",
       " 'behind',\n",
       " 'on',\n",
       " 'banning',\n",
       " 'people',\n",
       " 'Usually',\n",
       " 'this',\n",
       " 'http',\n",
       " '//imgur.com/MAqsCnN',\n",
       " 'is',\n",
       " 'enough')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.vocab.lookup(data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d32dc4",
   "metadata": {},
   "source": [
    "Check which words are in the vocabulary - <UNK> means unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe79838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('follow', '<UNK>', 'banned', '<UNK>')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.vocab.lookup([\"follow\", \"djfdsif\", \"banned\", \"recovered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ad07e",
   "metadata": {},
   "source": [
    "Shows the number and order of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21a1956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 106308 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d8ccf",
   "metadata": {},
   "source": [
    "Displays the count of a specific word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6e17750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts['fuck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c476b7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts['shit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00588868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts['rape']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3fe43",
   "metadata": {},
   "source": [
    "# generating sentences using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "284fa586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cold', 'there', 'too', '</s>', '</s>', '</s>', '</s>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e298de43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get', 'away', 'with', 'her', 'purse', 'and', 'punch']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(7, text_seed=['can'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44451db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'gif', 'is', 'looped', 'to']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(5, text_seed=['how'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886c451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
